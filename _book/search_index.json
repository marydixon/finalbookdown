[["index.html", "Environmental Data Science Bookdown Portfolio Chapter 1 Introduction 1.1 Structure 1.2 Weekly Topics", " Environmental Data Science Bookdown Portfolio Mary Dixon March 21, 2022 Chapter 1 Introduction SOCR580A7, Introduction to Environmental Data Science, covers R coding fundamentals in order to prepare graduate students to analyze environmental data. This bookdown document outlines the weekly topics and assignments of SOCR580A7. 1.1 Structure Each chapter represents an assignment from a week of course material. At the beginning of each chapter, there is code and material written by the professors, Drs. Matt Ross and Nathan Mueller. These materials are labelled Instructor Created Content. After the content written by the course instructors, the assignment for the week follows. This course began with workflow tools using R markdown and Github. The course progressed to cover data visualizations, functions, spatial analyses, linear models, and machine learning. 1.2 Weekly Topics Intro/Overview: Week 1, Jan 17-21. Environmental data basics, data types, file management (no assignment this week) Workflow Tools: Week 2, Jan 24-28. RMarkdown, Git, Github Data Wrangling and Key Programming Concepts: Week 3, Jan 31-Feb 4. Data munging, indexing, cleaning Functions and iterations: Week 4, Feb 7-11. Utility of functions, web scraping Debugging: Week 5, Feb 14-18. Overview of common errors, philosophy of troubleshooting (no assignment this week) Geospatial Analyses: Week 6, Feb 21-25. Spatial data analyses and mapping/visualization Linear Models: Week 7, Feb 28-Mar 4. Simple regression/multiple regression/timeseries/cross-sectional/panel models, API calls for data Introduction to Machine Learning: Week 8, Mar 7-11. Train, Validate, Test approach (private Loire River data, no chapter in bookdown) "],["workflow-tools.html", "Chapter 2 Workflow Tools 2.1 Instructor-Created Content 2.2 Assignment 1: Discharge Data, GitHub Website", " Chapter 2 Workflow Tools For the first week, we covered the basics of environmental data science. Nathan posted a video on an introduction to data science and data structure. We were introduced to the R Studio primers to reinforce statistical and R coding topics. The second week was when we had our first assignment. Matt posted a video on installing and using R and R Studio. This content covered the different components of what is found in the R Studio interface (environment, console, viewer). This week, we learned how to navigate GitHub including how to fork into a personal repository, clone to start a new R project, and push the R work to to GitHub. The instructors, Drs. Matt Ross and Nathan Mueller, wrote an Rmd script describing the Poudre River, how to download data using the readNWISdv() function, and how to make interactive graphs. The packages used this week: - tidyverse, a collection of R packages (dplyr, ggplot2, etc.) designed to wrangle and tidy data dataRetrieval, retrieves hydraulic and water quality data from USGS and EPA dygraphs, make interactive (zoom/pan/mouseover) plots of large datasets xts, creates and extensible time-series (xts) object from raw data inputs revealjs, composes Rmds into html presentations 2.1 Instructor-Created Content Methods The Poudre River at Lincoln Bridge is: Downstream of only a little bit of urban stormwater Near Odell Brewing CO Near an open space area and the Poudre River Trail Downstream of many agricultural diversions SiteDescription Data Acquisition and Plotting tests Data Download q &lt;- readNWISdv(siteNumbers = &#39;06752260&#39;, parameterCd = &#39;00060&#39;, startDate = &#39;2017-01-01&#39;, endDate = &#39;2022-01-01&#39;) %&gt;% rename(q = &#39;X_00060_00003&#39;) Static Data Plotter ggplot(q, aes(x = Date, y = q)) + geom_line() + ylab(&#39;Q (cfs)&#39;) + ggtitle(&#39;Discharge in the Poudre River, Fort Collins&#39;) Interactive Data Plotter q_xts &lt;- xts(q$q, order.by = q$Date) dygraph(q_xts) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) 2.2 Assignment 1: Discharge Data, GitHub Website This assignment will be primarily about demonstrating some expertise in using RMarkdown, since we will be using Rmds as the primary form of homework and assignments. Question 1 Fork the example repository into your personal GitHub The example is forked into my assignment 1 repository, r markdown examples. Question 2 Create an RStudio project from your Personal clone of the Repo. It has been cloned by copying the code in GitHub then opening RStudio and following the path of File &gt; New Project &gt; Version Control &gt; Git Question 3 Create a table of contents that is floating, but displays three levels of headers instead of two. The table of contents was adjusted to have three levels by defining toc_depth as 3 under the output section in the header. The third level subheadings are Data Download, Static Data Plotter, and Dynamic Data Plotter. 2.2.1 Question 4 Make a version of the dygraph with points and lines by using rstudios dygraph guide. {-} q_xts &lt;- xts(q$q, order.by = q$Date) dygraph(q_xts, main = &quot;Discharge DyGraph with Points&quot;) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) %&gt;% dyAxis(&quot;x&quot;, label = &quot;Date&quot;) %&gt;% dyOptions(drawPoints = TRUE, pointSize=3) Figure 2.1: Point graph showing discharge rate of the Poudre River in cfs from 2017 to 2021. q_xts &lt;- xts(q$q, order.by = q$Date) dygraph(q_xts, main = &quot;Discharge DyGraph with Lines&quot;) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) %&gt;% dyAxis(&quot;x&quot;, label = &quot;Date&quot;)%&gt;% dySeries(&quot;V1&quot;, strokeWidth = 2, strokePattern = &quot;dashed&quot;) Figure 2.2: Line graph showing discharge rate of the Poudre River in cfs from 2017 to 2021. An extensible time series object was created using the xts() function. This object was used to make an interactive plot using the dygraph() function. 2.2.2 Question 5 Writing a paragraph on the Poudre river with at least three hyperlinks, two bolded sections, and one italicized phrase. {-} The Cache la Poudre River begins in the Rocky Mountain National Park. Recreational activities along the Poudre River include camping, fishing, hiking, and scenic driving. To measure the amount of water flowing in this river, discharge data is collected. Discharge is measured by the equation, \\(discharge = A × v\\). Current discharge data is being impacted by ice at measurement sites. Discharge data is important because the Poudre River has flooded before. In 1864, a Poudre River flood destroyed Camp Collins, a military post. When the military relocated after this event, the camp was called Fort Collins. This camp became the city of Fort Collins we know today. 2.2.3 Question 6 Knit that document, and then git commit and push to your personal GitHub. {-} This document was saved, knitted, and then committed to GitHub by using the Git button in RStudio. 2.2.4 Question 7 Use the GitHub -&gt; Settings -&gt; Pages tab to create a website of your report. {-} The pages tab now shows my assignment published. 2.2.5 Bonus Question 1 Make the timestamp in the header dynamic. {-} The time stamp was changed to reflect todays date by adding format(Sys.time(), '%B %d, %Y') to the date section at the top of the markdown document. 2.2.6 Bonus Question 2 Create an index_talk.Rmd version of your document using the revealjs package. {-} The talk can be found here. "],["data-wrangling-and-key-programming-concepts.html", "Chapter 3 Data Wrangling and Key Programming Concepts 3.1 Instructor-Created Content 3.2 Assignment 2: Hayman Fire Recovery", " Chapter 3 Data Wrangling and Key Programming Concepts The emphasis of week three was data manipulation, starting with spatial data. Matt posted videos on the subject of the effects of the 2002 Hayman Fire at the Chessman Lake area. He showed students how to navigate Earth Engine Timelapse to look at changes from satellite images that have been recording data since 1984. From there, we navigated to Climate Engine, a tool to measure environmental data using satellite imagery. Through this website, we drew polygons to get normalized difference vegetation index (NDVI) data in areas that were burned and unburned. The instructors wrote an R script for this week outlining the basics of data munging. The instructor-created content includes downloaded and tidied environmental data (NDVI, NDSI, NDMI). The instructors visualized data through ggplot() functions. The mutate(), filter(), summarize(), and group_by() functions were some of the tools introduced this week to show how to manipulate and visualize data. The packages used this week: - tidyverse, a collection of R packages (dplyr, ggplot2, etc.) designed to wrangle and tidy data tidyr, part of tidyverse to organize data so that every column is variable, every row is an observation, and every cell is a single value ggthemes, change the appearance of ggplots lubridate, part of tidyverse, makes it easier to work with date-time data readr, part of tidyverse, reads in rectangular data 3.1 Instructor-Created Content Loading Packages Now that we have learned how to munge (manipulate) data and plot it, we will work on using these skills in new ways {-} Reading Files ####-----Reading in Data and Stacking it ----- #### #Reading in files files &lt;- list.files(&#39;data/&#39;,full.names=T) files ## [1] &quot;data/hayman_ndmi.csv&quot; &quot;data/hayman_ndsi.csv&quot; ## [3] &quot;data/hayman_ndvi.csv&quot; &quot;data/prismiowa.mat&quot; ## [5] &quot;data/PTSP_24hr.csv&quot; &quot;data/SASP_24hr.csv&quot; ## [7] &quot;data/SBB_SASP_Forcing_Data.txt&quot; &quot;data/SBB_SBSP_Forcing_Data.txt&quot; ## [9] &quot;data/SBSP_24hr.csv&quot; &quot;data/wq_sr.csv&quot; #Read in individual data files ndmi &lt;- read_csv(files[1]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndmi&#39;) ndsi &lt;- read_csv(files[2]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndsi&#39;) ndvi &lt;- read_csv(files[3])%&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndvi&#39;) # Stack as a tidy dataset full_long &lt;- rbind(ndvi,ndmi,ndsi) %&gt;% gather(key=&#39;site&#39;,value=&#39;value&#39;,-DateTime,-data) %&gt;% filter(!is.na(value)) 3.2 Assignment 2: Hayman Fire Recovery Question 1 What is the correlation between NDVI and NDMI? You should exclude winter months and focus on summer months. full_wide &lt;- spread(data=full_long,key=&#39;data&#39;,value=&#39;value&#39;) %&gt;% filter_if(is.numeric,all_vars(!is.na(.))) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) summer_only &lt;- filter(full_wide,month %in% c(6,7,8,9)) ggplot(summer_only,aes(x=ndmi,y=ndvi,color=site)) + geom_point(shape=1) + theme_few() + scale_color_few(labels = c(&quot;Burned&quot;, &quot;Unburned&quot;)) + theme(legend.position=c(0.8,0.8)) + theme(legend.title = element_blank()) + labs(x = &quot;NDMI&quot;, y = &quot;NDVI&quot;, title =&quot;Relationship between summertime normalized difference moisture index (NDMI) and normalized difference vegetation index (NDVI)&quot;) Figure 3.1: Relationship between summertime normalized difference vegetation index (NDVI) and normalized difference moisture index (NDMI) at the Cheesman Lake area in Colorado cor(summer_only$ndvi,summer_only$ndmi) ## [1] 0.6919666 The graph of NDVI to NDMI (3.1) indicates a positive correlation in which an increase in NDMI corresponds to an increase in NDVI. This conclusion is supported by the calculated correlation coefficient of NDVI to NDMI, R= 0.6919666. This strong positive correlation suggests that a rising moisture index corresponds to a rising vegetation index. Question 2 What is the correlation between average NDSI for January - April and average NDVI for June-August? Does the previous years snow cover influence vegetation growth for the following summer? annual_long_ndvi&lt;-filter(full_long,data==&quot;ndvi&quot;) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) %&gt;% filter(month %in% c(6,7,8)) %&gt;% group_by(site,year) %&gt;% summarize(mean_NDVI=mean(value)) annual_long_ndsi&lt;-filter(full_long,data==&quot;ndsi&quot;) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) %&gt;% filter(month %in% c(1,2,3,4)) %&gt;% group_by(site,year) %&gt;% summarize(mean_NDSI=mean(value)) ndsi_ndvi_annual &lt;- inner_join(annual_long_ndsi,annual_long_ndvi,by=c(&quot;year&quot;,&quot;site&quot;)) ggplot(ndsi_ndvi_annual,aes(x=mean_NDSI,y=mean_NDVI,col=site)) + geom_point() + theme_few() + theme(legend.position=c(0.85,0.3)) + theme(legend.title = element_blank()) + labs(x = &quot;Mean Winter NDSI&quot;, y = &quot;Mean Summer NDVI&quot;, title =&quot;Previous Year&#39;s Snow Index in Relation to Summer Vegetation Growth&quot;) + scale_color_few(labels = c(&quot;Burned&quot;, &quot;Unburned&quot;)) Figure 3.2: Correlation between average winter (January-April) normalized difference snow index (NDSI) and average summer (June-August) normalized difference vegetation index (NDVI) for the Cheesman Lake area in Colorado cor(ndsi_ndvi_annual$mean_NDSI,ndsi_ndvi_annual$mean_NDVI) ## [1] 0.1813135 Unlike the strong positive correlation we saw in the NDMI x NDVI graph (3.1) and calculated value (R=0.6919666), there is no strong effect for NDSI x NDVI (3.2). The spread of the data in the graph does not indicate a correlated direction. This conclusion is supported by the calculated correlation coefficient, R= 0.1813135. The correlation between these two values is slightly positive, but very weak. From the graph and calculated correlation coefficient, we can conclude that there is no strong relationship between mean winter NDSI and mean summer NDVI. Instead, the relationship is weak, and an increase in mean winter NDSI may correspond to a slight-to-no increase in mean summer NDVI. Question 3 How is the snow effect from question 2 different between pre- and post-burn and burned and unburned? ndvi_prepost &lt;- filter(full_long,data==&quot;ndvi&quot;) %&gt;% mutate(year = year(DateTime), month = month(DateTime), treatment = cut(year,breaks=c(0,2003,2020), labels=c(&#39;pre-burn&#39;,&#39;post-burn&#39;))) %&gt;% filter(month %in% c(6,7,8)) %&gt;% group_by(year,site,treatment) %&gt;% summarize(mean_ndvi = mean(value)) ndsi_prepost &lt;- filter(full_long,data==&quot;ndsi&quot;) %&gt;% mutate(year = year(DateTime), month = month(DateTime), treatment = cut(year,breaks=c(0,2003,2020), labels=c(&#39;pre-burn&#39;,&#39;post-burn&#39;))) %&gt;% filter(month %in% c(1,2,3,4)) %&gt;% group_by(year,site,treatment) %&gt;% summarize(mean_ndsi = mean(value)) ndsi_ndvi&lt;-inner_join(ndsi_prepost,ndvi_prepost,by=c(&quot;treatment&quot;,&quot;year&quot;,&quot;site&quot;)) ndsi_ndvi_mod &lt;- ndsi_ndvi %&gt;% mutate(site = recode(site, &#39;burned&#39; = &#39;Burned&#39;, &#39;unburned&#39; = &#39;Unburned&#39;)) %&gt;% mutate(treatment = recode(treatment, &#39;pre-burn&#39; = &#39;Pre-Burn&#39;, &#39;post-burn&#39;=&#39;Post-Burn&#39;)) ggplot(ndsi_ndvi_mod,aes(x=mean_ndsi,y=mean_ndvi,color=treatment)) + geom_point(shape=1) + geom_line() + theme_few() + labs(x = &quot;Mean Winter NDSI&quot;, y = &quot;Mean Summer NDVI&quot;, title = &quot;Relationship between winter normalized difference snow index (NDSI) on summer normalized difference vegetation index (NDVI) in burned and unburned sites&quot;) + theme(legend.title = element_blank()) + scale_color_few(labels = c(&quot;Before the Hayman Fire&quot;, &quot;After the Hayman Fire&quot;)) + theme(legend.position=c(0.8,0.2)) + facet_wrap(~site) Figure 3.3: Influence of winter normalized difference snow index (NDSI) on summer normalized difference vegetation index (NDVI) at the Cheesman Lake area of Colorado before the Hayman fire (blue) and after the Hayman fire (orange). ggplot(ndsi_ndvi_mod,aes(x=mean_ndsi,y=mean_ndvi,color=site)) + geom_point(shape=1) + geom_line() + theme_few() + scale_color_few(labels = c(&#39;Burned&#39;,&#39;Unburned&#39;)) + theme(legend.title = element_blank()) + theme(legend.position=c(0.37,0.2)) + labs(x = &quot;Mean Winter NDSI&quot;, y = &quot;Mean Summer NDVI&quot;, title = &quot;Relationship between winter normalized difference snow index (NDSI) on summer normalized difference vegetation index (NDVI) before and after the Hayman fire&quot;) + facet_wrap(~treatment) Figure 3.4: Influence of winter normalized difference snow index (NDSI) on summer normalized difference vegetation index (NDVI) in the burned sites (blue) and unburned sites (orange) at the Cheesman Lake area of Colorado. We saw a very weak but positive correlation between mean winter NDSI and mean summer NDVI in Q2 (3.2). We can see similar results from graphs illustrating the relationship between the burned/unburned (3.3) and pre/post-burn (3.4) mean winter NDSI and mean summer NDVI. The burned sites may have a slight positive correlation, but any correlation if present is very weak. The unburned sites appear to exhibit a slightly negative correlation, but still this correlation is very weak. The pre-burned sites do not appear correlated. The visualized data progress horizontally without an indication of either a positive or negative relationship. Similarly, the post-burned site do not have a strong correlation, but may have a slight positive correlation because the data seem to be slightly progressing in a positive direction. Ultimately, the correlations are weak between winter NDSI and summer NDVi regardless of site or treatment.None of the graphs showcase strong positive or negative correlations. Question 4 What month is the greenest month on average? month_ndvi &lt;- spread(data=full_long,key=&#39;data&#39;,value=&#39;value&#39;) %&gt;% filter_if(is.numeric,all_vars(!is.na(.))) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) %&gt;% group_by(site,month) %&gt;% summarize(mean_NDVI=mean(ndvi)) ggplot(month_ndvi,aes(x=factor(month),y=mean_NDVI,color=site)) + geom_point() + theme_few() + scale_color_few(labels = c(&#39;Burned&#39;,&#39;Unburned&#39;)) + theme(legend.title = element_blank()) + theme(legend.position=c(0.15,0.8)) + labs(x = &quot;Month&quot;, y = &quot;Mean NDVI&quot;, title =&quot;Average Normalized Difference Vegetation Index (NDVI) by Month&quot;)+ scale_x_discrete(labels=c(&#39;January&#39;,&#39;February&#39;,&#39;March&#39;,&#39;April&#39;,&#39;May&#39;,&#39;June&#39;,&#39;July&#39;,&#39;August&#39;,&#39;September&#39;,&#39;October&#39;,&#39;November&#39;,&#39;December&#39;)) + theme(axis.text.x = element_text(angle = 45,hjust=1)) Figure 3.5: Average NDVI by month for the burned (blue) and unburned (orange) sites. A high NDVI value corresponds to more greenness because it is a vegetation index. As seen in 3.5 September has the greatest mean NDVI value for the unburned site, closely followed by August. August had the greatest mean NDVI value for the burned site, closely followed by September. However, the decrease from August to September in the burned site is greater than the increase in greenness from August to September in the burned site. Therefore, August is the greenest month on average. Question 5 What month is the snowiest on average? month_ndsi &lt;- spread(data=full_long,key=&#39;data&#39;,value=&#39;value&#39;) %&gt;% filter_if(is.numeric,all_vars(!is.na(.))) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) %&gt;% group_by(site,month) %&gt;% summarize(mean_NDSI=mean(ndsi)) ggplot(month_ndsi,aes(x=factor(month),y=mean_NDSI,color=site)) + geom_point() + theme_few() + scale_color_few(labels=c(&#39;Burned&#39;,&#39;Unburned&#39;)) + theme(legend.title = element_blank()) + theme(legend.position=c(0.6,0.8)) + labs(x = &quot;Month&quot;, y = &quot;Mean NDSI&quot;, title =&quot;Average Normalized Difference Snow Index (NDSI) by Month&quot;) + scale_x_discrete(labels=c(&#39;January&#39;,&#39;February&#39;,&#39;March&#39;,&#39;April&#39;,&#39;May&#39;,&#39;June&#39;,&#39;July&#39;,&#39;August&#39;,&#39;September&#39;,&#39;October&#39;,&#39;November&#39;,&#39;December&#39;)) + theme(axis.text.x = element_text(angle = 45,hjust=1)) Figure 3.6: Average NDSI by month for the burned (blue) and unburned (orange) sites. As seen in 3.6, for the unburned site, February is the snowiest month, closely followed by January. For the burned site, January is the snowiest month, closely followed by February. The decrease in NDSI from January to February in the burned site is greater than the increase from January to February in the unburned site, so January is the snowiest month. Bonus Question 1 Redo all problems with spread and gather using modern tidyverse syntax. gather() was used to stack as a tidy dataset in the first section titled, Reading files # Stack as a tidy dataset **using tidyverse**. full_long_bonus &lt;- rbind(ndvi,ndmi,ndsi) %&gt;% pivot_longer(., cols=c(&quot;burned&quot;,&quot;unburned&quot;), names_to=&quot;site&quot;, values_to = &quot;value&quot;) %&gt;% filter(!is.na(value)) gather() was replaced by the pivot_longer() function spread() was used to make a full wide dataset in Question 1 full_wide_bonus &lt;- pivot_wider(full_long,names_from = &quot;data&quot;,values_from = &quot;value&quot;) %&gt;% filter_if(is.numeric,all_vars(!is.na(.))) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) summer_only_bonus &lt;- filter(full_wide,month %in% c(6,7,8,9)) ggplot(summer_only_bonus,aes(x=ndmi,y=ndvi,color=site)) + geom_point(shape=1) + theme_few() + scale_color_few(labels=c(&#39;Burned&#39;,&#39;Unburned&#39;)) + theme(legend.position=c(0.8,0.8)) + theme(legend.title = element_blank()) + labs(x = &quot;NDMI&quot;, y = &quot;NDVI&quot;, title =&quot;Relationship of normalized difference moisture index (NDMI) to normalized difference vegetation index (NDVI) During Summer Months&quot;) Figure 3.7: Relationship between summertime NDVI and NDMI at the Cheesman Lake area in Colorado cor(summer_only_bonus$ndvi,summer_only_bonus$ndmi) ## [1] 0.6919666 As we saw in Q1 (3.1), there is a positive correlation between these tested values. When there is an increase in NDMI, there is a concomitant increase in NDVI. spread() was replaced by the pivot_wider() function "],["functions-and-iterations.html", "Chapter 4 Functions and Iterations 4.1 Instructor-Created Content 4.2 Assignment 3: Snow Data", " Chapter 4 Functions and Iterations This fourth week of class focused on writing functions. Matt recorded and posted a video using data from the Poudre River to demonstrate the utility of functions. Matt reintroduced the readNWISdv function from the dataRetrieval package so that he could collect data from the USGS. Instead of rewriting code in a non-function oriented method, we can analyze data in a function-oriented method using maps and for loops. To make a function, we name it and set default parameters. Within the curly brackets, we define the functions. Matt created example functions to pull data from the Poudre River from different site numbers. After going over the basics of writing functions, we covered web scraping. We went to the Center for Snow and Avalanche Studies to pull in data. To collect this data, we looked for the html nodes and, with the function grepl(), used pattern matching to extract the data from the pattern we wanted to find. We looked at data from the Swamp Angel Study Plot and the Senator Beck Study Plot. The instructors this week, wrote code extract the csv files from a webpage using a for loop and a map function. The instructors also wrote example code to visualize the snow data in a ggplot. The packages used this week: tidyverse, a collection of R packages (dplyr, ggplot2, etc.) designed to wrangle and tidy data rvest, scrapes data from web pages lubridate, part of tidyverse, makes it easier to work with date-time data readxl, read data from Excel into R pdftools, helps with pdf utilities like text extraction and rendering 4.1 Instructor-Created Content Simple web scraping R can read html using either rvest, xml, or xml2 packages. Here we are going to navigate to the Center for Snow and Avalanche Studies Website and read a table in. This table contains links to data we want to programatically download for three sites. We dont know much about these sites, but they contain incredibly rich snow, temperature, and precipitation data. Reading an html Extract CSV links from webpage site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #See if we can extract tables and get the data that way tables &lt;- webpage %&gt;% html_nodes(&#39;table&#39;) %&gt;% magrittr::extract2(3) %&gt;% html_table(fill = TRUE) #That didn&#39;t work, so let&#39;s try a different approach #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;24hr&#39;,.)] %&gt;% html_attr(&#39;href&#39;) Data Download Download data in a for loop #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes file_names &lt;- paste0(&#39;data/&#39;,dataset) for(i in 1:3){ download.file(links[i],destfile=file_names[i]) } downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(downloaded) Download data in a map if(evaluate == T){ map2(links[1:3],file_names[1:3],download.file) }else{print(&#39;data already downloaded&#39;)} ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 0 Data read-in Read in just the snow data as a loop #Pattern matching to only keep certain files snow_files &lt;- file_names %&gt;% .[!grepl(&#39;SG_24&#39;,.)] %&gt;% .[!grepl(&#39;PTSP&#39;,.)] Read in the data as a map function our_snow_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_24hr.csv&#39;,&#39;&#39;,.) df &lt;- read_csv(file) %&gt;% select(Year,DOY,Sno_Height_M) %&gt;% mutate(site = name) } snow_data_full &lt;- map_dfr(snow_files,our_snow_reader) summary(snow_data_full) ## Year DOY Sno_Height_M site ## Min. :2003 Min. : 1.0 Min. :-3.523 Length:12786 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 Class :character ## Median :2012 Median :183.0 Median : 0.978 Mode :character ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 Plot snow data snow_yearly &lt;- snow_data_full %&gt;% group_by(Year,site) %&gt;% summarize(mean_height = mean(Sno_Height_M,na.rm=T)) ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + geom_point() + ggthemes::theme_few() + ggthemes::scale_color_few() 4.2 Assignment 3: Snow Data Question 1 Extract the meteorological data URLs. Here we want you to use the rvest package to get the URLs for the SASP forcing and SBSP_forcing meteorological datasets. site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; webpage &lt;- read_html(site_url) forcing_links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;forcing&#39;,.)] %&gt;% html_attr(&#39;href&#39;) Using the read_html() function, I read the html document from the website with the data we want to use, Snow Studies Archieved Data. Then, I used the grepl() function to extract data using pattern matching. We want data with with word forcing, so that term was used in the pattern matching function. Question 2 Download the meteorological data. Use the download_file and str_split_fixed commands to download the data and save it in your data folder. You can use a for loop or a map function. forcing_splits &lt;- str_split_fixed(forcing_links,&#39;/&#39;,8) forcing_dataset &lt;- forcing_splits[,8] forcing_file_names &lt;- paste0(&#39;data/&#39;,forcing_dataset) for(i in 1:2){ download.file(forcing_links[i],destfile=forcing_file_names[i]) } forcing_downloaded &lt;- file.exists(forcing_file_names) evaluate &lt;- !all(forcing_downloaded) The str_split_fixed() command was used to split the forcing links vector by /. The download.file() command was used in the for loop to download data. Question 3 Write a custom function to read in the data and append a site column to the data. headers &lt;- pdf_text(&#39;https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf&#39;) %&gt;% readr::read_lines(.) %&gt;% trimws(.) %&gt;% str_split_fixed(.,&#39;\\\\.&#39;,2) %&gt;% .[,2] %&gt;% .[1:20] %&gt;% str_trim(side = &quot;left&quot;) SBB_reader &lt;- function(file){ name = str_split_fixed(file,&#39;_&#39;,3)[,2] df &lt;- read.delim(file,header = FALSE,skip = 4,sep = &quot;&quot;) %&gt;% setNames(headers) %&gt;% mutate(site = name) } SBB_data_full &lt;- map_dfr(forcing_file_names,SBB_reader) A function called SBB_reader was created to read data. read.delim() was used to read the file names. mutate() was used to append a site column. Question 4 Use the map function to read in both meteorological files. Display a summary of your tibble. SBB_data_full_map &lt;- map_dfr(forcing_file_names,SBB_reader) summary(SBB_data_full_map) ## year month day hour minute ## Min. :2003 Min. : 1.000 Min. : 1.00 Min. : 0.00 Min. :0 ## 1st Qu.:2005 1st Qu.: 3.000 1st Qu.: 8.00 1st Qu.: 5.75 1st Qu.:0 ## Median :2007 Median : 6.000 Median :16.00 Median :11.50 Median :0 ## Mean :2007 Mean : 6.472 Mean :15.76 Mean :11.50 Mean :0 ## 3rd Qu.:2009 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:17.25 3rd Qu.:0 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :23.00 Max. :0 ## ## second precip [kg m-2 s-1] sw down [W m-2] lw down [W m-2] ## Min. :0 Min. :0.000e+00 Min. :-9999.000 Min. :-9999.0 ## 1st Qu.:0 1st Qu.:0.000e+00 1st Qu.: -3.510 1st Qu.: 173.4 ## Median :0 Median :0.000e+00 Median : -0.344 Median : 231.4 ## Mean :0 Mean :3.838e-05 Mean :-1351.008 Mean :-1325.7 ## 3rd Qu.:0 3rd Qu.:0.000e+00 3rd Qu.: 294.900 3rd Qu.: 272.2 ## Max. :0 Max. :6.111e-03 Max. : 1341.000 Max. : 365.8 ## ## air temp [K] windspeed [m s-1] relative humidity [%] pressure [Pa] ## Min. :242.1 Min. :-9999.000 Min. : 0.011 Min. :63931 ## 1st Qu.:265.8 1st Qu.: 0.852 1st Qu.: 37.580 1st Qu.:63931 ## Median :272.6 Median : 1.548 Median : 59.910 Median :65397 ## Mean :272.6 Mean : -790.054 Mean : 58.891 Mean :65397 ## 3rd Qu.:279.7 3rd Qu.: 3.087 3rd Qu.: 81.600 3rd Qu.:66863 ## Max. :295.8 Max. : 317.300 Max. :324.800 Max. :66863 ## ## specific humidity [g g-1] calculated dewpoint temperature [K] ## Min. :0.000000 Min. : 0.0 ## 1st Qu.:0.001744 1st Qu.: 0.0 ## Median :0.002838 Median : 0.0 ## Mean :0.003372 Mean : 74.9 ## 3rd Qu.:0.004508 3rd Qu.: 0.0 ## Max. :0.014780 Max. :2002.0 ## ## precip, WMO-corrected [kg m-2 s-1] ## Min. : 0.0 ## 1st Qu.: 0.0 ## Median : 0.0 ## Mean : 424.7 ## 3rd Qu.: 0.0 ## Max. :3002.0 ## ## air temp, corrected with Kent et al. (1993) [K] ## Min. : 0 ## 1st Qu.: 0 ## Median : 0 ## Mean : 438 ## 3rd Qu.: 0 ## Max. :5002 ## ## air temp, corrected with Anderson and Baumgartner (1998)[K] ## Min. : 0.0 ## 1st Qu.: 0.0 ## Median : 0.0 ## Mean : 173.9 ## 3rd Qu.: 0.0 ## Max. :5002.0 ## ## air temp, corrected with Nakamura and Mahrt (2005) [K] ## Min. : 0.0 ## 1st Qu.: 0.0 ## Median : 0.0 ## Mean : 605.9 ## 3rd Qu.: 0.0 ## Max. :6002.0 ## ## air temp, corrected with Huwald et al. (2009) [K] site ## Min. : 0.00 Length:138336 ## 1st Qu.: 0.00 Class :character ## Median : 0.00 Mode :character ## Mean : 56.49 ## 3rd Qu.: 0.00 ## Max. :6009.00 ## NA&#39;s :5214 The map_dfr() command was used to read in the data. Question 5 Make a line plot of mean temp by year by site (using the air temp [K] variable). Is there anything suspicious in the plot? Adjust your filtering if needed. SBB_yearly &lt;- SBB_data_full %&gt;% rename(temp = 10) %&gt;% group_by(year,site) %&gt;% summarize(mean_temp = mean(temp, na.rm = TRUE)) ggplot(SBB_yearly,aes(x=year,y=mean_temp,color=site)) + geom_line() + labs(x = &#39;Year&#39;, y = &#39;Mean Temperature (K)&#39;, title=&#39;Mean air temperature for the SASP and SBSP sites&#39;) + ggthemes::theme_few() + ggthemes::scale_color_few() + scale_x_continuous(breaks = c(2004, 2006, 2008, 2010)) + theme(legend.title = element_blank()) Figure 4.1: Mean air temperature for the Swamp Angel Study Plot (SASP) (blue) and Senator Beck Study Plot (SBSP) (orange) sites Question 6 Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Snow Angel Study Plot? temp_month &lt;- SBB_data_full %&gt;% filter(year %in% 2005:2010) %&gt;% group_by(month,site,year) %&gt;% summarize(mean_temp = mean(`air temp [K]`, na.rm = T)) ggplot(temp_month,aes(x = month,y = mean_temp, color = site)) + geom_line() + facet_wrap(~year) + labs(x = &#39;Month&#39;, y = &#39;Average Air Temperature (K)&#39;) + ggthemes::theme_few() + ggthemes::scale_color_few() + theme(legend.title = element_blank()) Figure 4.2: Average monthly temperatures (K) at each site for each year from 2005-2010 lineplotter &lt;- function(df){ monthdata &lt;- df %&gt;% filter(theyear==year)%&gt;% filter(year %in% 2005:2010) %&gt;% group_by(month,site,year) %&gt;% summarize(mean_temp = mean(`air temp [K]`, na.rm = T)) monthgraph &lt;- ggplot(monthdata,aes(x = month,y = mean_temp, color = site)) + geom_line() + ggtitle(theyear) + labs(x = &#39;Month&#39;, y = &#39;Average Temperature (K)&#39;) + ggthemes::theme_few() + ggthemes::scale_color_few() + theme(legend.title = element_blank()) print(monthgraph) } years &lt;- c(2005,2006,2007,2008,2009,2010) for (theyear in years){ lineplotter(SBB_data_full) } Figure 4.3: Average monthly temperatures (K) at each site for each year. Figure 4.4: Average monthly temperatures (K) at each site for each year. Figure 4.5: Average monthly temperatures (K) at each site for each year. Figure 4.6: Average monthly temperatures (K) at each site for each year. Figure 4.7: Average monthly temperatures (K) at each site for each year. Figure 4.8: Average monthly temperatures (K) at each site for each year. A function called lineplotter was created to plot monthly temperature data. This function was used in a for loop to plot these graphs for the years 2005-2010. The average monthly temperature of the Snow Angel Study Plot is warmer than the Senator Beck Study Plot every year from 2005-2010. Bonus Question 1 Make a plot of average daily precipitation by day of year (averaged across all available years). full_date &lt;- SBB_data_full %&gt;% mutate(date = make_date(year, month, day)) day_precip &lt;- full_date %&gt;% group_by(date, site) %&gt;% summarize(mean_precip = mean(`precip [kg m-2 s-1]`, na.rm = TRUE)) %&gt;% mutate(dayyr = lubridate::yday(date)) ggplot(day_precip, aes(x = dayyr,y = mean_precip)) + geom_line(colour = &#39;steelblue4&#39;) + labs(x = &#39;Day of Year&#39;, y = &#39;Mean Precipitation (kg/m^2/ s)&#39;, title=&#39;Average precipitation by the day of year at the SASP and SBSP&#39;) + ggthemes::theme_few() Figure 4.9: Average daily precipitation by day of year at the Swamp Angel Study Plot (SASP) and Senator Beck Study Plot (SBSP) The yday() command was using from the lubridate package to find day of year, and mutate() was used to append this information as a column. This column was then used as the x-axis in a ggplot. Bonus Question 2 Use a function and for loop to create yearly plots of precipitation by day of year. day_year_precip &lt;- full_date %&gt;% group_by(date, site, year) %&gt;% summarize(mean_precip = mean(`precip [kg m-2 s-1]`, na.rm = TRUE)) %&gt;% mutate(dayyr = lubridate::yday(date)) ggplot(day_year_precip, aes(x = dayyr,y = mean_precip)) + geom_line(colour=&#39;darkblue&#39;) + facet_wrap(~year) + labs(x = &#39;Day of Year&#39;, y = &#39;Mean Precipitation (kg/m^2 /s)&#39;) + ggthemes::theme_few() + ggthemes::scale_color_few() + theme(legend.title = element_blank()) Figure 4.10: Mean precipitation (kg/m2 /s) by day of year for each year from 2003-2011 at the Swamp Angel Study Plot (SASP) and the Senator Beck Study Plot (SBSP) precip_plotter &lt;- function(df){ daydata &lt;- df %&gt;% filter(theyear==year)%&gt;% group_by(date, site, year) %&gt;% summarize(mean_precip = mean(`precip [kg m-2 s-1]`, na.rm = TRUE)) %&gt;% mutate(dayyr = lubridate::yday(date)) daygraph &lt;- ggplot(daydata,aes(x = dayyr,y = mean_precip)) + geom_line(colour = &#39;steelblue&#39;) + ggtitle(theyear) + labs(x = &#39;Day of Year&#39;, y = &#39;Average Precipitation (kg/m^2/s&#39;) + ggthemes::theme_few() + ggthemes::scale_color_few() + theme(legend.title = element_blank()) print(daygraph) } years &lt;- c(2005,2006,2007,2008,2009,2010) for (theyear in years){ precip_plotter(full_date) } Figure 4.11: Mean precipitation (kg/m2 /s) by day of year at the Swamp Angel Study Plot (SASP) and the Senator Beck Study Plot (SBSP) Figure 4.12: Mean precipitation (kg/m2 /s) by day of year at the Swamp Angel Study Plot (SASP) and the Senator Beck Study Plot (SBSP) Figure 4.13: Mean precipitation (kg/m2 /s) by day of year at the Swamp Angel Study Plot (SASP) and the Senator Beck Study Plot (SBSP) Figure 4.14: Mean precipitation (kg/m2 /s) by day of year at the Swamp Angel Study Plot (SASP) and the Senator Beck Study Plot (SBSP) Figure 4.15: Mean precipitation (kg/m2 /s) by day of year at the Swamp Angel Study Plot (SASP) and the Senator Beck Study Plot (SBSP) Figure 4.16: Mean precipitation (kg/m2 /s) by day of year at the Swamp Angel Study Plot (SASP) and the Senator Beck Study Plot (SBSP) A function called precip_plotter was created to plot day-of-year precipitation data. This function was used in a for loop to plot these graphs for available years. "],["geospatial-data-analysis.html", "Chapter 5 Geospatial Data Analysis 5.1 Part 1 - LAGOS Spatial Analysis 5.2 Part 2 - Lake Water Quality Analysis", " Chapter 5 Geospatial Data Analysis The fifth week of SOCR580A7 focused on the philosophy of troubleshooting. In class, we worked through debugging examples and were given a practice Rmd with errors for us to fix. The instructors posted references to resources that may help, including the chapter on troubleshooting in the book, R for Graduate Students. In the sixth week of class, we began work on geospatial analysis. In the video he posted, Matt recommended the book by Lovelace, Nowosad, and Muenchow, Geocomputation with R. We worked with data from LAGOS, a geospatial database of lake water quality data. We discussed spatial data and how to convert from numeric latitude and longitude data to spatial geometry data using the function st_as_sf. This week, we used the mapview function to make interactive maps of lakes. Matt emphasized the importance of knowing the crs of this function. To find this information, we went to the EPSG WGS 84 website. From the large lake dataset we worked with this week, we learned to subset to certain states using the USAboundaries package. We also covered what it means when lakes have high or low values for chlorophyll-a concentration and secchi disk depths. The work for this is broken up in two parts, one for each assignment for this week. The first section is on general geospatial analysis and mapping using LAGOS lake data. The second section also uses LAGOS datasets, but focuses more on water quality attributes. The instructors wrote example code for downloading data into R from the LAGOS database. The instructors made various maps showing how to subset for states or number of observations. They introduced the st_join() function for spatial data, in contrast to non-spatial join functions, such as rbind(), inner_join(), full_join(), etc. The packages used in both parts this week: tidyverse, a collection of R packages (dplyr, ggplot2, etc.) designed to wrangle and tidy data sf, create shapefiles and encode spatial vector data mapview, create interactive visualizations of spatial data LAGOSNE, access lake water quality data through Lake Multi-scaled Geospatial and Temporal database USAboundaries, contains contemporary state, county, and Congressional district boundaries lubridate, part of tidyverse, makes it easier to work with date-time data 5.1 Part 1 - LAGOS Spatial Analysis 5.1.1 Instructor-Created Content LAGOS Analysis Loading in data First download and then specifically grab the locus (or site lat longs) # #Lagos download script #LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus Convert to spatial data #Look at the column names #names(lake_centers) #Look at the structure #str(lake_centers) #View the full dataset #View(lake_centers %&gt;% slice(1:100)) spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) %&gt;% st_transform(2163) #Subset for plotting subset_spatial &lt;- spatial_lakes %&gt;% slice(1:100) subset_baser &lt;- spatial_lakes[1:100,] #Dynamic mapviewer mapview(subset_spatial, canvas = T) Subset to only Minnesota states &lt;- us_states() #Plot all the states to check if they loaded #mapview(states) minnesota &lt;- states %&gt;% filter(name == &#39;Minnesota&#39;) %&gt;% st_transform(2163) #Subset lakes based on spatial position minnesota_lakes &lt;- spatial_lakes[minnesota,] %&gt;% mutate(state = &#39;Minnesota&#39;) #Plotting the first 1000 lakes minnesota_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;) 5.1.2 Assignment 4: LAGOS Spatial Analysis Question 1 Show a map outline of Iowa and Illinois. iowa &lt;- states %&gt;% filter(name == &#39;Iowa&#39;) %&gt;% st_transform(2163) illinois &lt;- states %&gt;% filter(name == &#39;Illinois&#39;) %&gt;% st_transform(2163) iowa_and_illinois &lt;- states %&gt;% filter(name==&#39;Iowa&#39; | name==&#39;Illinois&#39;) %&gt;% st_transform(2163) iowa_lakes &lt;- spatial_lakes[iowa,] %&gt;% mutate(state = &#39;Iowa&#39;) %&gt;% arrange(lake_area_ha) illinois_lakes &lt;- spatial_lakes[illinois,] %&gt;% mutate(state = &#39;Illinois&#39;)%&gt;% arrange(lake_area_ha) mapview(iowa_and_illinois) mapview(iowa_lakes, canvas = T, zcol = &#39;lake_area_ha&#39;, layer.name = &#39;Lake Area (ha)&#39;) mapview(illinois_lakes, canvas = T, zcol = &#39;lake_area_ha&#39;, layer.name = &#39;Lake Area (ha)&#39;) A general outline of Iowa and Illinois was created by subsetting the states dataset to include only Iowa and Illinois. This subset was then plotted using the mapview() function. Iowa and Illinois lakes were then plotted by subsetting the spatial_lakes dataset and plotting that information using mapview(). Question 2 Subset LAGOS data to these sites, how many sites are in Illinois and Iowa combined? How does this compare to Minnesota? iowa_illinois&lt;- rbind(iowa_lakes,illinois_lakes) nrow(iowa_illinois) ## [1] 16466 nrow(minnesota_lakes) ## [1] 29038 There are 16466 sites in Illinois and Iowa combined. This is 12572 fewer sites than Minnesota which has 29038 sites. Question 3 What is the distribution of lake size in Iowa vs. Minnesota? iowa_minnesota &lt;- rbind(iowa_lakes,minnesota_lakes) ggplot(iowa_minnesota, aes(lake_area_ha, fill = state)) + geom_histogram(position = &quot;dodge&quot;) + scale_x_log10() + theme_bw() + labs(x = &#39;Log Transformation of Lake Size (ha)&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of Lake Sizes in Iowa and Minnesota&#39;) + theme(legend.position=c(0.8,0.8)) + theme(legend.title = element_blank()) Figure 5.1: Distribution of Lake Sizes in Iowa (orange) and Minnesota (Blue) This side-by-side histogram shows the distribution of lake size in Iowa and Minnesota. Minnesota has more large lakes than Iowa. Qestions 4 Make an interactive plot of lakes in Iowa and Illinois and color them by lake area in hectares lake_area_states &lt;- iowa_illinois %&gt;% mutate(log10_lake_area = log(lake_area_ha)) %&gt;% arrange(log10_lake_area) mapview(lake_area_states, canvas = T, zcol = &#39;log10_lake_area&#39;, layer.name = &#39;Log 10 Transformation of Lake Area (ha)&#39;) An interactive plot was made to show lake size using the mapview() function. A log transformation was done to better illustrate differences in lake size. So the larger lakes werent hidden, the data were stacked in ascending order by the arrange() function. Question 5 What other data sources might we use to understand how reservoirs and natural lakes vary in size in these three states? We can look at Earth Engine to view a time lapse a body of water to see how it has changed over time. We can then use Climate Engine to make polygons (like we did for the Hayman fire recovery assignment) and use the remote sensing data to analyze the lakes. Images of lakes and reservoirs can be analyzed using an image analysis program, like Image J, to compare size. Canopeo is another open source image analysis software that is used to measure green area (usually used in leaf area index analysis). If images from Earth Engine were manipulated to have the lakes be colored green, then Canopeo can be used to measure the area. 5.2 Part 2 - Lake Water Quality Analysis 5.2.1 Instructor-Created Content LAGOS Analysis Loading in data First download and then specifically grab the locus (or site lat longs) #Lagos download script #lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus # Make an sf object spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) #Grab the water quality data nutr &lt;- lagos$epi_nutr #Look at column names #names(nutr) Subset columns nutr to only keep key info that we want clarity_only &lt;- nutr %&gt;% select(lagoslakeid,sampledate,chla,doc,secchi) %&gt;% mutate(sampledate = as.character(sampledate) %&gt;% ymd(.)) Keep sites with at least 200 observations #Look at the number of rows of dataset #nrow(clarity_only) chla_secchi &lt;- clarity_only %&gt;% filter(!is.na(chla), !is.na(secchi)) # How many observatiosn did we lose? # nrow(clarity_only) - nrow(chla_secchi) # Keep only the lakes with at least 200 observations of secchi and chla chla_secchi_200 &lt;- chla_secchi %&gt;% group_by(lagoslakeid) %&gt;% mutate(count = n()) %&gt;% filter(count &gt; 200) Join water quality data to spatial data spatial_200 &lt;- inner_join(spatial_lakes,chla_secchi_200 %&gt;% distinct(lagoslakeid,.keep_all=T), by=&#39;lagoslakeid&#39;) Mean Chl_a map ### Take the mean chl_a and secchi by lake mean_values_200 &lt;- chla_secchi_200 %&gt;% # Take summary by lake id group_by(lagoslakeid) %&gt;% # take mean chl_a per lake id summarize(mean_chl = mean(chla,na.rm=T), mean_secchi=mean(secchi,na.rm=T)) %&gt;% #Get rid of NAs filter(!is.na(mean_chl), !is.na(mean_secchi)) %&gt;% # Take the log base 10 of the mean_chl mutate(log10_mean_chl = log10(mean_chl)) #Join datasets mean_spatial &lt;- inner_join(spatial_lakes,mean_values_200, by=&#39;lagoslakeid&#39;) #Make a map mapview(mean_spatial,zcol=&#39;log10_mean_chl&#39;) 5.2.2 Assignmnet 5: Lake Water Quality Analysis Questions 1A What is the correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations? ggplot(mean_values_200,aes(x = mean_chl, y = mean_secchi)) + geom_point() + geom_smooth(method=&#39;lm&#39;, formula= y~x) + ggthemes::theme_few() + labs(x = &quot;Mean Chlorophyll (mg/L)&quot;, y = &quot;Mean Secchi Disk Depth (m)&quot;, title = &quot;Correlation between Secchi Disk Depth and Chlorophyll&quot;) Figure 5.2: Relationship between Secchi disk depth (m) and chlorophyll-a content (mg/L) in lakes with at least 200 observations. cor(mean_values_200$mean_chl,mean_values_200$mean_secchi) ## [1] -0.5320319 For sites with over 200 observations, there is a slight negative correlation between secchi disk depth and chlorophyll content. The correlation value is -0.5320319 which is in agreement with the graph that shows a slight negative relationship between these values. As mean chlorophyll value increases, the mean secchi depth decreases. Question 1B Why might this be the case? Secchi disk depth measures the clarity of the water. A greater secchi value indicates higher clarity of the water. Chlorophyll is a pigment found in plants, algae, and phytoplankton, so this measurement can approximate algae content in water. A higher chlorophyll content suggests reduced clarity. A high chlorophyll content would therefore correspond to lower secchi disk readings. Question 2A What states have the most data? First you will need to make a LAGOS spatial dataset that has the total number of counts per site. spatial_lakes &lt;-lake_centers %&gt;% group_by(lagoslakeid,nhd_long,nhd_lat) %&gt;% count() %&gt;% st_as_sf(.,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) The count() function was used to count unique values in the spatial_lakes dataset. Question 2B Second, you will need to join this point dataset to the us_boundaries data. spatial_statelakes&lt;- st_join(spatial_lakes,us_states()) The st_join() function was used to join these datasets. Question 2C Then you will want to group by state and sum all the observations in that state and arrange that data from most to least total observations per state. state_counts &lt;- spatial_statelakes %&gt;% as.data.frame() %&gt;% # (remove geospatial data) select(-geometry) %&gt;% #(removes geometry column) group_by(name) %&gt;% summarize(statecount = sum(n)) %&gt;% arrange(desc(statecount)) state_counts[1,1] ## # A tibble: 1 x 1 ## name ## &lt;chr&gt; ## 1 Minnesota The state with the most data is Minnesota. Question 3 Is there a spatial pattern in Secchi disk depth for lakes with at least 200 observations? secchi_200 &lt;- clarity_only %&gt;% group_by(lagoslakeid) %&gt;% mutate(count = n()) %&gt;% filter(count &gt; 200) mean_secchi_200 &lt;- secchi_200 %&gt;% group_by(lagoslakeid) %&gt;% summarize(mean_secchi=mean(secchi,na.rm=T)) %&gt;% filter(!is.na(mean_secchi)) %&gt;% mutate(log10_mean_secchi = log10(mean_secchi)) spatial_secchi_200 &lt;- inner_join(spatial_lakes,mean_secchi_200 %&gt;% distinct(lagoslakeid,.keep_all=T), by=&#39;lagoslakeid&#39;) mapview(spatial_secchi_200, canvas = TRUE, zcol = &#39;mean_secchi&#39;, layer.name = &#39;Mean Secchi Depth&#39;) There does seem to be a spatial pattern to the secchi disk depth. The New England states have higher secchi disk depth readings. Farther west, in states like Minnesota and Missouri, the secchi disk depth readings decrease. "],["linear-models.html", "Chapter 6 Linear Models 6.1 Instructor-Created Content 6.2 Assignment 6: Weather and Corn Yield Regressions", " Chapter 6 Linear Models The focus of the seventh week was on linear models. Nathan showed us how to create a USDA NASS API key. This key allows us to pull data from the USDA NASS into R using the function, rnassqs(). We used this data on weather and corn yield to analyze relationships between sets of parameters. These parameters were used in linear models. We discussed what the different components of a linear model are how to add quadratic trends to linear models. We learned that the p-values for the specific parameters indicate the level of significance for that parameter and the R2 value represents the percentage of variability explained by the model. The instructors wrote example code this week to pull in MATLAB files. The instructors also wrote code to pull in the USDA NASS data into R without having to navigate the NASS QuickStats website and download csv files. These data files of temperature and crop yield were combined and tidied. Nathan showed in his R code how to how to assign dimension names to a matrix and how to convert a matrix into a data frame. The packages used this week: tidyverse, a collection of R packages (dplyr, ggplot2, etc.) designed to wrangle and tidy data R.matlab, read in MATLAB file rnassqs, access to USDA NASS quick stats API to download data directly from USDA into R ggthemes, change the look of ggplots 6.1 Instructor-Created Content Weather Data Analysis Load the PRISM daily maximum temperatures # daily max temperature # dimensions: counties x days x years prism &lt;- readMat(&quot;data/prismiowa.mat&quot;) # look at county #1 t_1981_c1 &lt;- prism$tmaxdaily.iowa[1,,1] t_1981_c1[366] ## [1] NaN plot(1:366, t_1981_c1, type = &quot;l&quot;) ggplot() + geom_line(mapping = aes(x=1:366, y = t_1981_c1)) + theme_bw() + xlab(&quot;day of year&quot;) + ylab(&quot;daily maximum temperature (°C)&quot;) + ggtitle(&quot;Daily Maximum Temperature, Iowa County #1&quot;) ## Warning: Removed 1 row(s) containing missing values (geom_path). # assign dimension names to tmax matrix dimnames(prism$tmaxdaily.iowa) &lt;- list(prism$COUNTYFP, 1:366, prism$years) #wants a list corresponding to different dimensions - first dim is county code (FP code) # converted 3d matrix into a data frame tmaxdf &lt;- as.data.frame.table(prism$tmaxdaily.iowa) # relabel the columns colnames(tmaxdf) &lt;- c(&quot;countyfp&quot;,&quot;doy&quot;,&quot;year&quot;,&quot;tmax&quot;) tmaxdf &lt;- tibble(tmaxdf) Temperature trends Summer temperature trends: Winneshiek County tmaxdf$doy &lt;- as.numeric(tmaxdf$doy) tmaxdf$year &lt;- as.numeric(as.character(tmaxdf$year)) winnesummer &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ggplot(winnesummer, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; lm_summertmax &lt;- lm(meantmax ~ year, winnesummer) summary(lm_summertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnesummer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5189 -0.7867 -0.0341 0.6859 3.7415 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.57670 36.44848 1.141 0.262 ## year -0.00747 0.01823 -0.410 0.684 ## ## Residual standard error: 1.232 on 36 degrees of freedom ## Multiple R-squared: 0.004644, Adjusted R-squared: -0.02301 ## F-statistic: 0.168 on 1 and 36 DF, p-value: 0.6844 Winter Temperatures - Winneshiek County winnewinter &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; doy &lt;= 59 | doy &gt;= 335 &amp; !is.na(tmax)) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ggplot(winnewinter, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) lm_wintertmax &lt;- lm(meantmax ~ year, winnewinter) summary(lm_wintertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnewinter) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.0748 -2.6494 0.7715 2.2172 4.1361 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -119.10546 94.08124 -1.266 0.214 ## year 0.05960 0.04705 1.267 0.213 ## ## Residual standard error: 3.181 on 36 degrees of freedom ## Multiple R-squared: 0.04267, Adjusted R-squared: 0.01608 ## F-statistic: 1.605 on 1 and 36 DF, p-value: 0.2134 Multiple regression  Quadratic time trend winnewinter$yearsq &lt;- winnewinter$year^2 lm_wintertmaxquad &lt;- lm(meantmax ~ year + yearsq, winnewinter) summary(lm_wintertmaxquad) ## ## Call: ## lm(formula = meantmax ~ year + yearsq, data = winnewinter) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.680 -2.813 0.640 2.264 4.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.049e+04 1.939e+04 -0.541 0.592 ## year 1.043e+01 1.940e+01 0.538 0.594 ## yearsq -2.594e-03 4.851e-03 -0.535 0.596 ## ## Residual standard error: 3.213 on 35 degrees of freedom ## Multiple R-squared: 0.05043, Adjusted R-squared: -0.003832 ## F-statistic: 0.9294 on 2 and 35 DF, p-value: 0.4043 winnewinter$fitted &lt;- lm_wintertmaxquad$fitted.values ggplot(winnewinter) + geom_point(mapping = aes(x = year, y = meantmax)) + geom_line(mapping = aes(x = year, y = fitted)) + theme_bw() + labs(x = &quot;year&quot;, y = &quot;tmax&quot;) Download NASS corn yield data # set our API key with NASS nassqs_auth(key = &quot;08EB8353-3696-30D9-96D4-839C4DEA18B4&quot;) # parameters to query on params &lt;- list(commodity_desc = &quot;CORN&quot;, util_practice_desc = &quot;GRAIN&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) # download cornyieldsall &lt;- nassqs_yields(params) cornyieldsall$county_ansi &lt;- as.numeric(cornyieldsall$county_ansi) cornyieldsall$yield &lt;- as.numeric(cornyieldsall$Value) # clean and filter this dataset cornyields &lt;- select(cornyieldsall, county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) cornyields &lt;- tibble(cornyields) 6.2 Assignment 6: Weather and Corn Yield Regressions Question 1A Extract Winneshiek County corn yields, fit a linear time trend, make a plot. Is there a significant time trend? winnecorn &lt;- cornyields %&gt;% filter(county_ansi==191) ggplot(winnecorn, mapping = aes(x = year, y = yield)) + geom_point() + labs(x = &quot;Year&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Winneshiek Corn Yield over Time&quot;) + geom_smooth(method = lm) + theme_bw() Figure 6.1: Linear time trend of Winneshiek County corn yields (bu/acre) lm_winnecorn &lt;- lm(yield ~ year, winnecorn) summary(lm_winnecorn) ## ## Call: ## lm(formula = yield ~ year, data = winnecorn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.163 -1.841 2.363 9.437 24.376 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4763.290 448.286 -10.63 4.46e-13 *** ## year 2.457 0.224 10.96 1.77e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.97 on 39 degrees of freedom ## Multiple R-squared: 0.7551, Adjusted R-squared: 0.7488 ## F-statistic: 120.2 on 1 and 39 DF, p-value: 1.767e-13 From our linear model (6.1), we can see that there is a significant time trend. The p value for the year is less than alpha = 0.05. This agrees with the graph showing the yield data following the linear model line in a positive slope. With increasing years, there is a concomitant increase in yield. Question 1B Fit a quadratic time trend (i.e., year + year^2) and make a plot. Is there evidence for slowing yield growth? winnecorn$yearsq &lt;- winnecorn$year^2 lm_winnecornquad &lt;- lm(yield ~ year + yearsq, winnecorn) summary(lm_winnecornquad) ## ## Call: ## lm(formula = yield ~ year + yearsq, data = winnecorn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.384 -3.115 1.388 9.743 25.324 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.583e+04 8.580e+04 0.301 0.765 ## year -2.812e+01 8.576e+01 -0.328 0.745 ## yearsq 7.641e-03 2.143e-02 0.357 0.723 ## ## Residual standard error: 17.17 on 38 degrees of freedom ## Multiple R-squared: 0.7559, Adjusted R-squared: 0.7431 ## F-statistic: 58.84 on 2 and 38 DF, p-value: 2.311e-12 winnecorn$fitted &lt;- lm_winnecornquad$fitted.values ggplot(winnecorn) + geom_point(mapping = aes(x = year, y = yield)) + geom_line(mapping = aes(x = year, y = fitted)) + theme_bw() + labs(x = &quot;Year&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Quadratic Time Trend for Corn Yield&quot;) Figure 6.2: Line graph with points showing a quadratic time progression of corn yield in Winneshiek County. ggplot(winnecorn) + geom_point(mapping = aes(x = year, y = yield)) + geom_smooth(mapping = aes(x = year, y = fitted), method = lm) + theme_bw() + labs(x = &quot;Year&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Quadratic Time Trend for Corn Yield&quot;) Figure 6.3: Quadratic time trend with year and year2 as predictors for corn yield in Winneshiek County. There is not sufficient evidence for slowing yield growth. The year squared estimate is positive and the year is negative, indicating that yield growth is not slowing. The p values for the individual estimates are also greater than alpha = 0.05. Question 2 Time Series: Lets analyze the relationship between temperature and yields for the Winneshiek County time series. Use data on yield and summer avg Tmax. Is adding year or Tmax^2 to your model helpful? Make a plot and interpret the results. Combine data for yield and temperature in Winneshiek County: summercorn &lt;- inner_join(winnecorn,winnesummer, by = &#39;year&#39;) summercorn$meantmaxsq &lt;- summercorn$meantmax^2 Make linear regression models with the different parameters of interest: lm_summercorn_single &lt;- lm(yield ~ meantmax, summercorn) summary(lm_summercorn_single) ## ## Call: ## lm(formula = yield ~ meantmax, data = summercorn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -71.96 -19.85 -3.19 24.64 61.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 275.876 118.335 2.331 0.0255 * ## meantmax -4.763 4.438 -1.073 0.2902 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.88 on 36 degrees of freedom ## Multiple R-squared: 0.03101, Adjusted R-squared: 0.004098 ## F-statistic: 1.152 on 1 and 36 DF, p-value: 0.2902 summercorn$fittedsingle &lt;- lm_summercorn_single$fitted.values lm_summercorn_quad &lt;- lm(yield ~ meantmax + meantmaxsq, summercorn) summary(lm_summercorn_quad) ## ## Call: ## lm(formula = yield ~ meantmax + meantmaxsq, data = summercorn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.587 -22.262 -0.982 22.409 52.798 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4223.604 1446.639 -2.920 0.00609 ** ## meantmax 328.918 107.068 3.072 0.00410 ** ## meantmaxsq -6.173 1.979 -3.119 0.00362 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.5 on 35 degrees of freedom ## Multiple R-squared: 0.2417, Adjusted R-squared: 0.1984 ## F-statistic: 5.579 on 2 and 35 DF, p-value: 0.007887 summercorn$fittedquad &lt;- lm_summercorn_quad$fitted.values lm_summercorn_year &lt;- lm(yield ~ meantmax + year, summercorn) summary(lm_summercorn_year) ## ## Call: ## lm(formula = yield ~ meantmax + year, data = summercorn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.071 -7.269 2.271 9.935 27.505 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4791.774 513.812 -9.326 5.10e-11 *** ## meantmax -3.201 2.308 -1.387 0.174 ## year 2.514 0.253 9.934 1.01e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.06 on 35 degrees of freedom ## Multiple R-squared: 0.7463, Adjusted R-squared: 0.7318 ## F-statistic: 51.48 on 2 and 35 DF, p-value: 3.761e-11 summercorn$fittedyear &lt;- lm_summercorn_year$fitted.values lm_summercorn &lt;- lm(yield ~ meantmax + year + meantmaxsq, summercorn) summary(lm_summercorn) ## ## Call: ## lm(formula = yield ~ meantmax + year + meantmaxsq, data = summercorn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.683 -5.427 1.214 8.943 25.127 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7280.2956 755.2808 -9.639 2.96e-11 *** ## meantmax 208.9004 52.9773 3.943 0.000381 *** ## year 2.3286 0.2166 10.752 1.77e-12 *** ## meantmaxsq -3.9259 0.9799 -4.006 0.000318 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.27 on 34 degrees of freedom ## Multiple R-squared: 0.8277, Adjusted R-squared: 0.8125 ## F-statistic: 54.43 on 3 and 34 DF, p-value: 4.538e-13 summercorn$fitted &lt;- lm_summercorn$fitted.values Plot the linear model results ggplot(summercorn) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_line(mapping = aes(x = meantmax, y = fittedsingle)) + theme_bw() + geom_smooth(mapping = aes(x = meantmax, y = fittedsingle), method = lm) + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Corn Yield and Mean Maximum Summer Temperature&quot;) Figure 6.4: Mean maximum summer temperature (°C) of Winneshiek County as a predictor of corn yield (bu/acre) ggplot(summercorn) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_line(mapping = aes(x = meantmax, y = fittedquad)) + theme_bw() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Quadratic Temperature Trend for Corn Yield&quot;) Figure 6.5: Quadratic time trend (year2 ) of Winneshiek County as a predictor of corn yield (bu/acre) ggplot(summercorn) + geom_point(mapping = aes(x = meantmax, y = yield)) + theme_bw() + geom_smooth(mapping = aes(x = meantmax, y = fittedquad), method = lm) + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Quadratic Temperature Trend for Corn Yield&quot;) Figure 6.6: Quadratic temperature trend (mean maximum temperature (°C)2 ) of Winneshiek County as a predictor of corn yield (bu/acre) ggplot(summercorn) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_smooth(mapping = aes(x = meantmax, y = fittedyear),method = lm) + theme_bw() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Years and Max Temperature as Corn Yield Predictors&quot;) Figure 6.7: Time in years and mean maximum temperature (°C) of Winneshiek County as a predictors of corn yield (bu/acre) ggplot(summercorn) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_smooth(mapping = aes(x = meantmax, y = fitted),method = lm) + theme_bw() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Years, Max Temperature, and Quadratic Temperature as Corn Yield Predictors&quot;) Figure 6.8: Time in years, mean maximum temperature (°C), and quadratic temperature of Winneshiek County as a predictors of corn yield (bu/acre) Adding tmax2 and year is helpful to the model. From the simple linear regression, we saw that the R-squared value is 0.03101, indicating that the mean max temperature observation only explains 3.101% of the variability in yield. When tmax^2 was added, the R-squared value increased to 0.1984. When year was added, R-sqaured increased to 0.7318. When year and tmax2 are added, the R-squared value changed to 0.8125, indicating that 81.25% of the variation within yield is explained by all predictors. These results suggest that adding year and tmax^2 make a stronger model. The estimate for mean max temperature squared is negative, indicating that temperature change is slowing. With increasing maximum temperature, there is a decrease in yield. With increasing years, there is an increase in corn yield. Question 3 Cross-Section: Analyze the relationship between temperature and yield across all counties in 2018. Is there a relationship? Interpret the results. Select data from 2018 and combine yield and temperature data: yield &lt;- cornyieldsall %&gt;% filter(year == 2018) %&gt;% group_by(county_name) %&gt;% unique() %&gt;% filter(!is.na(county_ansi)) temp &lt;- tmaxdf %&gt;% group_by(countyfp) %&gt;% filter(year == 2018) %&gt;% filter(doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% summarize(meantmax = mean(tmax)) %&gt;% rename(county_ansi = &quot;countyfp&quot;) temp$county_ansi &lt;- as.numeric(as.character(temp$county_ansi)) summeryield &lt;- left_join(yield, temp, by=&#39;county_ansi&#39;) %&gt;% select(., county_ansi, county_name, yield, meantmax) Fit a simple and multiple regression model: lm_summeryield &lt;- lm(yield ~ meantmax, summeryield) summary(lm_summeryield) ## ## Call: ## lm(formula = yield ~ meantmax, data = summeryield) ## ## Residuals: ## Min 1Q Median 3Q Max ## -42.983 -15.041 0.955 16.795 31.999 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 312.466 63.387 4.929 3.69e-06 *** ## meantmax -4.216 2.241 -1.882 0.0631 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.64 on 91 degrees of freedom ## Multiple R-squared: 0.03745, Adjusted R-squared: 0.02687 ## F-statistic: 3.541 on 1 and 91 DF, p-value: 0.06308 summeryield$meantmaxsq &lt;- summeryield$meantmax^2 lm_summeryield_quad &lt;- lm(yield ~ meantmax + meantmaxsq, summeryield) summary(lm_summeryield_quad) ## ## Call: ## lm(formula = yield ~ meantmax + meantmaxsq, data = summeryield) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.221 -15.399 5.007 14.541 30.879 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5501.602 1860.830 -2.957 0.00397 ** ## meantmax 406.789 131.493 3.094 0.00263 ** ## meantmaxsq -7.256 2.321 -3.126 0.00239 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.75 on 90 degrees of freedom ## Multiple R-squared: 0.1317, Adjusted R-squared: 0.1124 ## F-statistic: 6.827 on 2 and 90 DF, p-value: 0.001736 summeryield$fitted &lt;- lm_summeryield_quad$fitted.values Plot results of models: ggplot(summeryield, mapping = aes(x = meantmax, y = yield)) + geom_point() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Corn Yield and Mean Maximum Summer Temperature in 2018&quot;) + geom_smooth(method = lm) + theme_bw() Figure 6.9: Mean Maximum Summer Temperature (°C) as a predictor of corn Yield (bu/acre) in all Iowa counties in 2018. ggplot(summeryield) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_line(mapping = aes(x = meantmax, y = fitted)) + theme_bw() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Quadratic Temperature Trend for Corn Yield in 2018&quot;) Figure 6.10: 2018 corn yield (bu/acre) in all Iowa counties. The line shows temperature in a quadratic progression (mean maximum summer temperature (°C)2 ) and points show mean maximum summer temperature (°C). ggplot(summeryield) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_smooth(mapping = aes(x = meantmax, y = fitted), method = lm) + theme_bw() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Quadratic Temperature Trend for Corn Yield in 2018&quot;) Figure 6.11: Quadratic temperature trend (mean maximum summer temperature (°C)2 ) as a predictor of corn Yield (bu/acre) in all Iowa counties in 2018. From the graph of temperature and yield across all counties (6.11), we can see that there is a slight negative progression among the data. As temperature increases, yield decreases. The estimate from the simple linear model is negative, which supports the illustration in the graph. However, the p value from the simple linear regression is greater than alpha = 0.05, so we do not have sufficient evidence to conclude that there is a relationship. When mean temp max squared was added as an estimate in the model, the significance of the temperature predictors changed to less than alpha = 0.05. We can conclude from this model with temp and temp^2 as predictors, that there is a relationship between temperature and yield. Question 4: Panel: Convert the county ID code into factor using as.factor(), then include this variable in a regression using all counties yield and summer temperature data. How does the significance of your temperature coefficients (Tmax, Tmax2 ) change? Make a plot comparing actual and fitted yields and interpret the results of your model. Convert county ID into factor and join temperature and yield data: yieldall &lt;- cornyieldsall %&gt;% group_by(county_name) %&gt;% unique() %&gt;% filter(!is.na(county_ansi)) tempall &lt;- tmaxdf %&gt;% group_by(countyfp) %&gt;% filter(doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% summarize(meantmax = mean(tmax)) %&gt;% rename(county_ansi = &quot;countyfp&quot;) tempall$county_ansi &lt;- as.numeric(as.character(tempall$county_ansi)) summeryieldall &lt;- left_join(yieldall, tempall, by=&#39;county_ansi&#39;) %&gt;% select(., county_ansi, county_name, yield, meantmax, year) summeryieldall$county_ansi &lt;- as.factor(summeryieldall$county_ansi) Run simple and multiple regression models: summeryieldall$meantmaxsq &lt;- summeryieldall$meantmax^2 lm_summeryieldall &lt;- lm(yield ~ meantmax + meantmaxsq, summeryieldall) summary(lm_summeryieldall) ## ## Call: ## lm(formula = yield ~ meantmax + meantmaxsq, data = summeryieldall) ## ## Residuals: ## Min 1Q Median 3Q Max ## -121.002 -22.522 -0.525 26.442 86.550 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3680.191 844.714 -4.357 1.35e-05 *** ## meantmax 285.830 60.642 4.713 2.52e-06 *** ## meantmaxsq -5.325 1.088 -4.894 1.02e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.29 on 4012 degrees of freedom ## Multiple R-squared: 0.0491, Adjusted R-squared: 0.04862 ## F-statistic: 103.6 on 2 and 4012 DF, p-value: &lt; 2.2e-16 lm_summeryield_panel &lt;- lm(yield ~ meantmax + meantmaxsq + county_ansi, summeryieldall) summary(lm_summeryield_panel) ## ## Call: ## lm(formula = yield ~ meantmax + meantmaxsq + county_ansi, data = summeryieldall) ## ## Residuals: ## Min 1Q Median 3Q Max ## -115.150 -22.076 -0.895 25.952 82.566 ## ## Coefficients: (2 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.131e+03 5.398e+03 -1.506 0.13206 ## meantmax 6.110e+02 3.926e+02 1.556 0.11971 ## meantmaxsq -1.126e+01 7.133e+00 -1.578 0.11454 ## county_ansi3 -5.811e+00 7.676e+00 -0.757 0.44905 ## county_ansi5 -6.793e+00 6.844e+00 -0.992 0.32105 ## county_ansi7 -1.623e+01 8.253e+00 -1.967 0.04930 * ## county_ansi9 -8.711e-01 7.185e+00 -0.121 0.90351 ## county_ansi11 -3.433e+00 7.612e+00 -0.451 0.65202 ## county_ansi13 -4.205e+00 7.649e+00 -0.550 0.58257 ## county_ansi15 7.454e+00 7.158e+00 1.041 0.29774 ## county_ansi17 1.167e-01 7.456e+00 0.016 0.98751 ## county_ansi19 -2.918e+00 7.323e+00 -0.398 0.69028 ## county_ansi21 -2.440e+00 7.683e+00 -0.318 0.75084 ## county_ansi23 -1.161e+00 7.690e+00 -0.151 0.87995 ## county_ansi25 -1.806e-02 7.650e+00 -0.002 0.99812 ## county_ansi27 2.105e+00 7.317e+00 0.288 0.77361 ## county_ansi29 7.371e+00 7.484e+00 0.985 0.32477 ## county_ansi31 5.293e+00 7.495e+00 0.706 0.48010 ## county_ansi33 -6.324e+00 7.010e+00 -0.902 0.36703 ## county_ansi35 3.255e+00 7.632e+00 0.427 0.66975 ## county_ansi37 -7.138e+00 6.879e+00 -1.038 0.29944 ## county_ansi39 -2.494e+01 8.065e+00 -3.092 0.00200 ** ## county_ansi41 -5.117e+00 7.647e+00 -0.669 0.50343 ## county_ansi43 -2.358e+00 7.095e+00 -0.332 0.73960 ## county_ansi45 4.344e-01 7.583e+00 0.057 0.95432 ## county_ansi47 -4.662e-01 7.433e+00 -0.063 0.94999 ## county_ansi49 8.485e+00 7.137e+00 1.189 0.23453 ## county_ansi51 -3.678e+00 1.234e+01 -0.298 0.76561 ## county_ansi53 -1.479e+01 8.834e+00 -1.674 0.09413 . ## county_ansi55 -1.107e+00 7.030e+00 -0.158 0.87485 ## county_ansi57 1.363e+01 8.795e+00 1.549 0.12139 ## county_ansi59 -9.503e+00 7.165e+00 -1.326 0.18479 ## county_ansi61 -3.571e-01 6.970e+00 -0.051 0.95914 ## county_ansi63 -4.546e+00 6.945e+00 -0.655 0.51274 ## county_ansi65 -2.783e+00 6.867e+00 -0.405 0.68530 ## county_ansi67 -6.171e+00 7.338e+00 -0.841 0.40041 ## county_ansi69 -3.550e-01 7.598e+00 -0.047 0.96274 ## county_ansi71 4.210e+01 2.262e+01 1.862 0.06275 . ## county_ansi73 1.060e+01 6.991e+00 1.516 0.12966 ## county_ansi75 7.736e-01 7.678e+00 0.101 0.91974 ## county_ansi77 1.762e+00 7.078e+00 0.249 0.80347 ## county_ansi79 2.387e+00 7.541e+00 0.317 0.75156 ## county_ansi81 -1.474e+00 6.932e+00 -0.213 0.83160 ## county_ansi83 2.638e+00 7.629e+00 0.346 0.72948 ## county_ansi85 9.046e+00 8.676e+00 1.043 0.29715 ## county_ansi87 1.063e+01 9.601e+00 1.107 0.26822 ## county_ansi89 -2.675e+00 9.051e+00 -0.296 0.76757 ## county_ansi91 -1.429e+00 7.322e+00 -0.195 0.84531 ## county_ansi93 1.744e+00 7.641e+00 0.228 0.81945 ## county_ansi95 -2.068e-02 7.073e+00 -0.003 0.99767 ## county_ansi97 -1.294e+01 7.622e+00 -1.697 0.08976 . ## county_ansi99 9.297e+00 7.064e+00 1.316 0.18818 ## county_ansi101 1.561e+01 1.485e+01 1.051 0.29331 ## county_ansi103 -2.953e+00 7.084e+00 -0.417 0.67680 ## county_ansi105 -5.033e+00 7.734e+00 -0.651 0.51521 ## county_ansi107 2.862e+00 7.820e+00 0.366 0.71439 ## county_ansi109 6.575e-01 7.222e+00 0.091 0.92746 ## county_ansi111 1.749e+01 1.488e+01 1.175 0.23988 ## county_ansi113 -5.180e+00 7.716e+00 -0.671 0.50201 ## county_ansi115 1.442e+01 1.007e+01 1.432 0.15209 ## county_ansi117 -2.228e+01 8.094e+00 -2.753 0.00593 ** ## county_ansi119 2.612e-01 7.526e+00 0.035 0.97231 ## county_ansi121 -3.731e+00 7.171e+00 -0.520 0.60294 ## county_ansi123 8.888e+00 7.583e+00 1.172 0.24125 ## county_ansi125 1.991e+00 7.735e+00 0.257 0.79689 ## county_ansi127 3.870e+00 7.700e+00 0.503 0.61528 ## county_ansi129 2.855e+01 1.652e+01 1.728 0.08406 . ## county_ansi131 -5.815e-02 7.260e+00 -0.008 0.99361 ## county_ansi133 8.667e+00 9.621e+00 0.901 0.36776 ## county_ansi135 -1.747e+01 7.455e+00 -2.344 0.01915 * ## county_ansi137 1.680e+01 1.111e+01 1.512 0.13071 ## county_ansi139 7.588e+00 7.370e+00 1.030 0.30325 ## county_ansi141 3.578e+00 7.680e+00 0.466 0.64133 ## county_ansi143 -2.783e+00 7.080e+00 -0.393 0.69430 ## county_ansi145 1.119e+01 1.140e+01 0.982 0.32629 ## county_ansi147 -4.178e+00 7.624e+00 -0.548 0.58377 ## county_ansi149 1.951e+00 7.078e+00 0.276 0.78284 ## county_ansi151 4.138e-01 7.721e+00 0.054 0.95726 ## county_ansi153 1.348e+01 7.406e+00 1.820 0.06886 . ## county_ansi155 1.546e+01 8.682e+00 1.780 0.07508 . ## county_ansi157 5.593e+00 6.988e+00 0.800 0.42352 ## county_ansi159 -1.981e+01 7.825e+00 -2.532 0.01137 * ## county_ansi161 -5.455e-01 7.676e+00 -0.071 0.94335 ## county_ansi163 7.564e+00 7.326e+00 1.032 0.30190 ## county_ansi165 3.609e-01 7.242e+00 0.050 0.96026 ## county_ansi167 8.327e+00 7.282e+00 1.144 0.25287 ## county_ansi169 1.817e-01 7.603e+00 0.024 0.98094 ## county_ansi171 -9.328e-01 7.608e+00 -0.123 0.90243 ## county_ansi173 -9.599e+00 8.955e+00 -1.072 0.28380 ## county_ansi175 -1.234e+01 7.352e+00 -1.678 0.09336 . ## county_ansi177 2.064e+01 2.129e+01 0.969 0.33236 ## county_ansi179 3.311e+00 1.016e+01 0.326 0.74458 ## county_ansi181 -3.579e+00 7.771e+00 -0.461 0.64509 ## county_ansi183 1.459e+01 9.259e+00 1.576 0.11510 ## county_ansi185 -2.220e+01 7.476e+00 -2.970 0.00300 ** ## county_ansi187 3.868e+00 7.664e+00 0.505 0.61381 ## county_ansi189 -1.066e+00 6.842e+00 -0.156 0.87621 ## county_ansi191 -3.217e+00 7.150e+00 -0.450 0.65273 ## county_ansi193 1.477e+00 7.009e+00 0.211 0.83310 ## county_ansi195 NA NA NA NA ## county_ansi197 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 34.65 on 3916 degrees of freedom ## Multiple R-squared: 0.1048, Adjusted R-squared: 0.08245 ## F-statistic: 4.68 on 98 and 3916 DF, p-value: &lt; 2.2e-16 summeryieldall$fitted &lt;- lm_summeryield_panel$fitted.values Plot actual and fitted yields: ggplot(summeryieldall) + geom_point(mapping = aes(x = yield, y = fitted)) + geom_smooth(mapping = aes(x = yield, y = fitted),method = lm) + theme_few() + labs(x = &quot;Actual Yield&quot;, y = &quot;Fitted Yield&quot;, title = &quot;Fitted Versus Actual Corn Yields For Summer&quot;) Figure 6.12: Fitted and actual summer corn yields (bu/acre) for all Iowa counties. Our panel regression model has an adjusted R-squared value of 0.08245, so only 8.245% of the variation within yield is explained by predictors. The mean max temperature and mean max temperature squared have p values greater than alpha = 0.05, so we do not have sufficient evidence to conclude that there is a relationship between these temperature values and corresponding yield. The significance of the temperature coefficients changed when we added the county ansi to the model. When the linear model included mean max temperature and mean max temperature squared, the p values were less than alpha = 0.05, indicating statistical significance. When county ansi was added, the p values for the temperature coefficients increased to greater than alpha = 0.05, indicating a lack of statistical significance. Question 5 Soybeans: Download NASS data on soybean yields and explore either a time series relationship for a given county, the cross-sectional relationship for a given year, or a panel across all counties and years. Download soybean data: soybeanparams &lt;- list(commodity_desc = &quot;SOYBEANS&quot;, statisticcat_desc= &quot;YIELD&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) soyall &lt;- nassqs_yields(soybeanparams) soyall$county_ansi &lt;- as.numeric(soyall$county_ansi) soyall$yield &lt;- as.numeric(soyall$Value) soyyield &lt;- select(soyall, county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) soyyield &lt;- tibble(soyyield) Cross-sectional relationship of soybean yield and mean summer max temperature for the year 2015: soyyield2015 &lt;- soyall %&gt;% filter(year == 2015) %&gt;% group_by(county_name) %&gt;% unique() %&gt;% filter(!is.na(county_ansi)) soytemp2015 &lt;- tmaxdf %&gt;% group_by(countyfp) %&gt;% filter(year == 2015) %&gt;% filter(doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% summarize(meantmax = mean(tmax)) %&gt;% rename(county_ansi = &quot;countyfp&quot;) soytemp2015$county_ansi &lt;- as.numeric(as.character(soytemp2015$county_ansi)) soy2015 &lt;- left_join(soyyield2015, soytemp2015, by=&#39;county_ansi&#39;) %&gt;% select(., county_ansi, county_name, yield, meantmax) lm_soyyield &lt;- lm(yield ~ meantmax, soy2015) summary(lm_soyyield) ## ## Call: ## lm(formula = yield ~ meantmax, data = soy2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.0568 -2.4976 0.2312 1.9743 9.3812 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 166.5497 16.0416 10.382 &lt; 2e-16 *** ## meantmax -4.1582 0.6013 -6.915 5.38e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.657 on 95 degrees of freedom ## Multiple R-squared: 0.3348, Adjusted R-squared: 0.3278 ## F-statistic: 47.82 on 1 and 95 DF, p-value: 5.384e-10 soy2015$meantmaxsq &lt;- soy2015$meantmax^2 lm_soy2015_quad &lt;- lm(yield ~ meantmax + meantmaxsq, soy2015) summary(lm_soy2015_quad) ## ## Call: ## lm(formula = yield ~ meantmax + meantmaxsq, data = soy2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8646 -2.3046 0.4521 1.5551 8.7845 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -995.3104 603.6188 -1.649 0.1025 ## meantmax 82.7546 45.1420 1.833 0.0699 . ## meantmaxsq -1.6245 0.8437 -1.925 0.0572 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.606 on 94 degrees of freedom ## Multiple R-squared: 0.36, Adjusted R-squared: 0.3464 ## F-statistic: 26.44 on 2 and 94 DF, p-value: 7.744e-10 soy2015$fitted &lt;- lm_soy2015_quad$fitted.values ggplot(soy2015, mapping = aes(x = meantmax, y = yield)) + geom_point() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Soybean Yield and Mean Maximum Summer Temperature&quot;) + geom_smooth(method = lm) + theme_bw() Figure 6.13: Mean maximum summer temperature (°C) in 2015 as a predictor for soybean yield (bu/acre) for all Iowa counties. ggplot(soy2015) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_line(mapping = aes(x = meantmax, y = fitted)) + theme_bw() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Soybean Yield and Quadratic Trend of Mean Maximum Summer Temperature&quot;) Figure 6.14: Quadratric temperature trend (mean maximum summer temperature (°C)2 ) in 2015 and soybean yield (bu/acre) for all Iowa counties. ggplot(soy2015) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_smooth(mapping = aes(x = meantmax, y = fitted), method = lm) + theme_bw() + labs(x = &quot;Mean Max Temperature (°C)&quot;, y = &quot;Yield (bu/acre)&quot;, title = &quot;Soybean Yield and Quadratic Trend of Mean Maximum Summer Temperature&quot;) Figure 6.15: Quadratric temperature trend (mean maximum summer temperature (°C)2 ) in 2015 as a predictor for soybean yield (bu/acre) for all Iowa counties. A simple linear regression of mean summer temperature and yield gives an R-squared value of 0.3348. We get a negative estimate of mean max summer temperature, indicating that as mean temperature decreases, yield increases. This finding is supported by the graph showing the relationship between yield and mean max temperature. This graph shows the data moving along a negative slope. The p value for the mean max temperature estimate is less than alpha = 0.05, suggesting we have enough evidence to claim a relationship between the predictor and yield. However, when we add mean max summer temperature squared to the model, the p value changes for each predictor to be greater than alpha = 0.05, so we cannot claim to have sufficient evidence to reject the null hypothesis using this model. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
